Requested:
    1. Describe any data manipulation you performed
    2. Describe the algorithm you chose and the results
    3. Describe the performances of the model


Data Manipulation:
1. Categorical variables were converted to boolean (coded as integer 0/1).

2. Specifically, the field "years_on_file" which is a text field with strings that look like:
    "between 12 and 14 years" (18 different categories in general) was converted to a set of boolean variables, each
        one representing that information is relevant "up to" a certain number of years (rounded up).

    For example, "between 12 and 14 years" was interpreted as "14", and converted to a boolean True for the variables
        representing 0.5,1,1.5,2,2.5,3,5,6,8,10,12,14 and 0 for the variables representing 17,20,25,30,35,50 .
        e.g: "years_file_upto_12".
    The logic is that the information is overlapping: Having information up to 14 years also means having information
        on 12,5, etc.
    "Over 35" was interpreted as "between 35 and 50" to maintain a coherent structure.
    The interpretation was done by parsing the string for the higher number.

3. For columns which contained NaN, a new boolean column was created indicating if value is Nan.
    The NaN values were imputed to be 0 (under the assumption that the learning algorithm will learn to use the
        corresponding "missing value" data in that case.

4. "pit" (Point in Time) field was parsed and translated to "Number of days since the earliest day in the data",
    for convenience.

5. For clients (represented by client_id) which had several entries, used only the last one.
    This is in order to prevent data leakage, where the user ends up in both the train and the test set, possibly
     more than once.

6. Unnecessary fields were deleted before the learning phase.

7. The values were scaled linearly using MinMaxScaler to be in the range 0..1
    Mean 0 was not used so as not to disturb the 0 value with its special meaning.



The algorithms chosen:
----------------------

First, as per recommendation in the brief, the data was split based on the 'period'
 variable, with a model for each value.
The data was randomly split to a 60% train set and 40% test set.


After that, two classification algorithms from scikitlearn were used to infer the result.
 * A simple LogisticRegression
 * A fancy GradientBoostingClassifier


As a baseline, the same algorithms and methods were tested to infer from a single attribute, which is the
 output of the current model (field "Current_model_probability").


The results:
----------------------

Model for first_funded+120:
------------------------
    Results for Logistic Regression:
                Baseline                                   |         Full Data
                  precision    recall  f1-score   support  |            precision    recall  f1-score   support
             0.0       0.93      0.99      0.96      1075  |     0.0       0.96      0.99      0.97      1075
             1.0       0.90      0.61      0.73       200  |     1.0       0.92      0.78      0.85       200

    Results for GradientBoostingClassifier:
                Baseline                                    |        Full Data
                  precision    recall  f1-score   support   |           precision    recall  f1-score   support
             0.0       0.94      0.97      0.96      1075   |     0.0       0.96      0.98      0.97      1075
             1.0       0.83      0.69      0.75       200   |     1.0       0.89      0.80      0.84       200


Model for first_funded+180:
------------------------
    Results for Logistic Regression:
                Baseline                                   |         Full Data
                  precision    recall  f1-score   support  |             precision    recall  f1-score   support
             0.0       0.94      0.98      0.96      1064  |     0.0       0.95      0.99      0.97      1064
             1.0       0.87      0.74      0.80       242  |     1.0       0.93      0.76      0.84       242

    Results for GradientBoostingClassifier:
                Baseline                                   |         Full Data
                  precision    recall  f1-score   support  |             precision  recall  f1-score   support
             0.0       0.94      0.96      0.95      1064  |     0.0       0.96      0.98      0.97      1064
             1.0       0.82      0.73      0.77       242  |     1.0       0.91      0.84      0.87       242


Performance (results-wise):
---------------------------
The results seem somewhat too high - but that is only for the case of 0 (i.e. no loan default), which is the most common
 case (the data is skewed, as can be seen, only about 16% of the cases are defaulting). This is the more important
 case  - the (rarer) case of defaulting, and is analyzed below.

For the baseline, Logistic Regression performed roughly the same on 120, and better on 180.
For the full data, XGB performed better on both cases.

The two sets are indeed different, in the 120-day case most of the improvement is in recall, while in the 180-day case,
 most the the improvement is in precision.

The f1 score of the baseline is 0.77/0.80 while the score
of the new algorithm 0.76/0.84 for each set.

That is an improvement over the baseline.


Performance (time-wise):
---------------------------
The total code running time was 34 sec.
This is almost entirely due to the slow performance of the method add_years_boolean which calculates the
 boolean variables for the "years_on_file" variable.
I cached the results of this method in an external Pickle file for quicker performance during iterations,
    and they changed significantly when using the cache.

Before:
    Time spent in (Main) is:  34.81769962757546
After:
    Time spent in (Main) is:  2.0391585030484927

Note, this time also includes calculating and printing some exploratory data.
